```rust
TokenSheetData {
    tokens: [
        TokenData::Keyword(
            Keyword::Form(
                Gn,
            ),
        ),
        TokenData::Ident(
            `some_neural_network`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::LeftDelimiter(
                    Delimiter::Par,
                ),
            ),
        ),
        TokenData::Ident(
            `input`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::Colon,
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::LeftDelimiter(
                    Delimiter::Box,
                ),
            ),
        ),
        TokenData::Literal(
            LiteralTokenData::Integer(
                UnspecifiedRegular(
                    3,
                ),
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::RightDelimiter(
                    Delimiter::Box,
                ),
            ),
        ),
        TokenData::Ident(
            `f32`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::RightDelimiter(
                    Delimiter::Par,
                ),
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::Binary(
                    SynBinaryOpr::CurryType,
                ),
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::LeftDelimiter(
                    Delimiter::Box,
                ),
            ),
        ),
        TokenData::Literal(
            LiteralTokenData::Integer(
                UnspecifiedRegular(
                    3,
                ),
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::RightDelimiter(
                    Delimiter::Box,
                ),
            ),
        ),
        TokenData::Ident(
            `f32`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::Colon,
            ),
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::ColonHyphen,
            ),
        ),
        TokenData::Ident(
            `begin`,
        ),
        TokenData::Ident(
            `session`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::ColonHyphen,
            ),
        ),
        TokenData::Ident(
            `end`,
        ),
        TokenData::Ident(
            `session`,
        ),
        TokenData::Ident(
            `by`,
        ),
        TokenData::Ident(
            `sgd`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::LeftDelimiter(
                    Delimiter::Par,
                ),
            ),
        ),
        TokenData::Ident(
            `loss`,
        ),
        TokenData::Punctuation(
            Punctuation(
                PunctuationMapped::RightDelimiter(
                    Delimiter::Par,
                ),
            ),
        ),
        TokenData::Keyword(
            Keyword::Todo,
        ),
    ],
    token_verses: TokenVerses {
        main_sequence: MainTokenVerseSequence {
            verses_data: [
                TokenVerseData {
                    start: TokenVerseStart(
                        TokenIdx(
                            1,
                        ),
                    ),
                    indent: 0,
                },
                TokenVerseData {
                    start: TokenVerseStart(
                        TokenIdx(
                            17,
                        ),
                    ),
                    indent: 4,
                },
                TokenVerseData {
                    start: TokenVerseStart(
                        TokenIdx(
                            20,
                        ),
                    ),
                    indent: 4,
                },
                TokenVerseData {
                    start: TokenVerseStart(
                        TokenIdx(
                            28,
                        ),
                    ),
                    indent: 4,
                },
            ],
        },
        nested_sequences: [],
    },
}
```