@article{bahdanau2014neural,
  added-at = {2021-04-29T12:01:03.000+0200},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/20d6b4e471c01b5a1d0be959d2f74494d/nilsd},
  interhash = {bb2ca011eeafccb0bd2505c9476dcd10},
  intrahash = {0d6b4e471c01b5a1d0be959d2f74494d},
  journal = {arXiv preprint arXiv:1409.0473},
  keywords = {},
  timestamp = {2021-04-29T12:01:03.000+0200},
  title = {Neural machine translation by jointly learning to align and translate},
  year = 2014
}
@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@inproceedings{DBLP:conf/naacl/DevlinCLT19,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}
@inproceedings{NEURIPS2021_7f489f64,
 author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15084--15097},
 publisher = {Curran Associates, Inc.},
 title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{NEURIPS2021_099fe6b0,
 author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1273--1286},
 publisher = {Curran Associates, Inc.},
 title = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{DBLP:journals/corr/abs-2108-07732,
  author       = {Jacob Austin and
                  Augustus Odena and
                  Maxwell I. Nye and
                  Maarten Bosma and
                  Henryk Michalewski and
                  David Dohan and
                  Ellen Jiang and
                  Carrie J. Cai and
                  Michael Terry and
                  Quoc V. Le and
                  Charles Sutton},
  title        = {Program Synthesis with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2108.07732},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07732},
  eprinttype    = {arXiv},
  eprint       = {2108.07732},
  timestamp    = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2108-07732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
Lample2020Deep,
title={Deep Learning For Symbolic Mathematics},
author={Guillaume Lample and François Charton},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1eZYeHFDS}
}
@inproceedings{
raghu2021do,
title={Do Vision Transformers See Like Convolutional Neural Networks?},
author={Maithra Raghu and Thomas Unterthiner and Simon Kornblith and Chiyuan Zhang and Alexey Dosovitskiy},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=R-616EWWKF5}
}
@inproceedings{
Cordonnier2020On,
title={On the Relationship between Self-Attention and Convolutional Layers},
author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJlnC1rKPB}
}
@misc{
li2022can,
title={Can Vision Transformers Perform Convolution?},
author={Shanda Li and Xiangning Chen and Di He and Cho-Jui Hsieh},
year={2022},
url={https://openreview.net/forum?id=W2gO9bYYG5P}
}
@misc{bubeck2023sparks,
title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
year={2023},
eprint={2303.12712},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
@inproceedings{Bhattamishra2020OnTA,
  title={On the Ability and Limitations of Transformers to Recognize Formal Languages},
  author={S. Bhattamishra and Kabir Ahuja and Navin Goyal},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222225236}
}
@inproceedings{
feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=qHrADgAdYu}
}
@InProceedings{pmlr-v202-giannou23a,
  title =    {Looped Transformers as Programmable Computers},
  author =       {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-Yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  booktitle =    {Proceedings of the 40th International Conference on Machine Learning},
  pages =    {11398--11442},
  year =   {2023},
  editor =   {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume =   {202},
  series =   {Proceedings of Machine Learning Research},
  month =    {23--29 Jul},
  publisher =    {PMLR},
  pdf =    {https://proceedings.mlr.press/v202/giannou23a/giannou23a.pdf},
  url =    {https://proceedings.mlr.press/v202/giannou23a.html},
  abstract =   {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.}
}
@inproceedings{NIPS1989_53c3bce6,
 author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Handwritten Digit Recognition with a Back-Propagation Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
 volume = {2},
 year = {1989}
}
@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}
@article{Yin2017ComparativeSO,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Wenpeng Yin and Katharina Kann and Mo Yu and Hinrich Sch{\"u}tze},
  journal={ArXiv},
  year={2017},
  volume={abs/1702.01923},
  url={https://api.semanticscholar.org/CorpusID:17400045}
}
@InProceedings{Chefer_2021_CVPR,
    author    = {Chefer, Hila and Gur, Shir and Wolf, Lior},
    title     = {Transformer Interpretability Beyond Attention Visualization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {782-791}
}
@article{Chollet2016XceptionDL,
  title={Xception: Deep Learning with Depthwise Separable Convolutions},
  author={François Chollet},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1800-1807},
  url={https://api.semanticscholar.org/CorpusID:2375110}
}
@inproceedings{
jelassi2022vision,
title={Vision Transformers provably learn spatial structure},
author={Samy Jelassi and Michael Eli Sander and Yuanzhi Li},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=eMW9AkXaREI}
}
@ARTICLE{1166689,
  author={Fessler, J.A. and Sutton, B.P.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Nonuniform fast Fourier transforms using min-max interpolation}, 
  year={2003},
  volume={51},
  number={2},
  pages={560-574},
  keywords={Fast Fourier transforms;Interpolation;Frequency domain analysis;Approximation error;Iterative methods;Image reconstruction;Multidimensional signal processing;Nonuniform sampling;Multidimensional systems;Magnetic resonance imaging},
  doi={10.1109/TSP.2002.807005}}
@ARTICLE{502315,
  author={Bagchi, S. and Mitra, S.K.},
  journal={IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing}, 
  title={The nonuniform discrete Fourier transform and its applications in filter design. I. 1-D}, 
  year={1996},
  volume={43},
  number={6},
  pages={422-433},
  keywords={Discrete Fourier transforms;Finite impulse response filter;Sampling methods;Signal processing algorithms;Frequency;Algorithm design and analysis;Design methodology;Fast Fourier transforms;Chirp},
  doi={10.1109/82.502315}}
@ARTICLE{502316,
author={Bagchi, S. and Mitra, S.K.},
journal={IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing}, 
title={The nonuniform discrete Fourier transform and its applications in filter design. II. 2-D}, 
year={1996},
volume={43},
number={6},
pages={434-444},
keywords={Discrete Fourier transforms;Finite impulse response filter;Sampling methods;Shape;Passband;Nonuniform sampling;Two dimensional displays;Frequency domain analysis;Design methodology;Fourier transforms},
doi={10.1109/82.502316}}
@inproceedings{
Yun2020Are,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}
@inproceedings{
luo2022your,
title={Your Transformer May Not be as Powerful as You Expect},
author={Shengjie Luo and Shanda Li and Shuxin Zheng and Tie-Yan Liu and Liwei Wang and Di He},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=NQFFNdsOGD}
}
@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}
@INPROCEEDINGS{9880273,
  author={Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs}, 
  year={2022},
  volume={},
  number={},
  pages={11953-11965},
  keywords={Convolutional codes;Shape;Scalability;Transformers;Data models;Pattern recognition;Convolutional neural networks;Deep learning architectures and techniques},
  doi={10.1109/CVPR52688.2022.01166}}
@article{Tan2019EfficientNetRM,
  title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Mingxing Tan and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.11946},
  url={https://api.semanticscholar.org/CorpusID:167217261}
}
@INPROCEEDINGS{9578487,
  author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V.},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Meta Pseudo Labels}, 
  year={2021},
  volume={},
  number={},
  pages={11552-11563},
  keywords={Computer vision;Semisupervised learning;Benchmark testing;Pattern recognition;Standards},
  doi={10.1109/CVPR46437.2021.01139}}
  @misc{liu2024sora,
      title={Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models}, 
      author={Yixin Liu and Kai Zhang and Yuan Li and Zhiling Yan and Chujie Gao and Ruoxi Chen and Zhengqing Yuan and Yue Huang and Hanchi Sun and Jianfeng Gao and Lifang He and Lichao Sun},
      year={2024},
      eprint={2402.17177},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@INPROCEEDINGS{10377858,
  author={Peebles, William and Xie, Saining},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Scalable Diffusion Models with Transformers}, 
  year={2023},
  volume={},
  number={},
  pages={4172-4182},
  keywords={Computer vision;Computational modeling;Scalability;Computer architecture;Benchmark testing;Transformers;Complexity theory},
  doi={10.1109/ICCV51070.2023.00387}}
@article{adelson1984pmi,
  added-at = {2011-09-19T12:12:54.000+0200},
  author = {Adelson, E. H. and Anderson, C. H. and Bergen, J. R. and Burt, P. J. and Ogden, J. M.},
  biburl = {https://www.bibsonomy.org/bibtex/259dfac6a273a879eb5c33f0f5b740980/sac},
  citeulike-article-id = {1622723},
  interhash = {1b86abb78a10e821d19471cbc87bbe0e},
  intrahash = {59dfac6a273a879eb5c33f0f5b740980},
  journal = {RCA Engineer},
  keywords = {deepzoom image ma10 processing pyramid},
  number = 6,
  pages = {33--41},
  posted-at = {2007-09-05 11:12:27},
  priority = {0},
  timestamp = {2011-09-19T12:12:54.000+0200},
  title = {{1984, Pyramid methods in image processing}},
  volume = 29,
  year = 1984
}
@article{SU2024127063,
title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
journal = {Neurocomputing},
volume = {568},
pages = {127063},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127063},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.}
}
@book{apple,
 ISBN = {9780691080789},
 URL = {http://www.jstor.org/stable/j.ctt1bpm9w6},
 abstract = {The authors present a unified treatment of basic topics that arise in Fourier analysis. Their intention is to illustrate the role played by the structure of Euclidean spaces, particularly the action of translations, dilatations, and rotations, and to motivate the study of harmonic analysis on more general spaces having an analogous structure, e.g., symmetric spaces.},
 author = {Elias M. Stein and Guido Weiss},
 publisher = {Princeton University Press},
 title = {Introduction to Fourier Analysis on Euclidean Spaces (PMS-32)},
 urldate = {2024-05-13},
 year = {1971}
}
@article{DBLP:journals/corr/abs-2009-07485,
  author       = {Hossein Gholamalinezhad and
                  Hossein Khosravi},
  title        = {Pooling Methods in Deep Neural Networks, a Review},
  journal      = {CoRR},
  volume       = {abs/2009.07485},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.07485},
  eprinttype    = {arXiv},
  eprint       = {2009.07485},
  timestamp    = {Wed, 23 Sep 2020 15:51:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-07485.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/CzarneckiOJSP17,
  author       = {Wojciech Marian Czarnecki and
                  Simon Osindero and
                  Max Jaderberg and
                  Grzegorz Swirszcz and
                  Razvan Pascanu},
  title        = {Sobolev Training for Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1706.04859},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.04859},
  eprinttype    = {arXiv},
  eprint       = {1706.04859},
  timestamp    = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/CzarneckiOJSP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{lu2022sobolev,
      title={Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent}, 
      author={Yiping Lu and Jose Blanchet and Lexing Ying},
      year={2022},
      eprint={2205.07331},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}
@book{evans10,
  abstract = {"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography."--Publisher's description.},
  added-at = {2022-10-12T17:19:51.000+0200},
  address = {Providence, R.I.},
  author = {Evans, Lawrence C.},
  biburl = {https://www.bibsonomy.org/bibtex/2f5b120723ea78913e7e700ddd1a99301/annakrause},
  interhash = {59982ce44cc43813ccb14c0d647a59ee},
  intrahash = {f5b120723ea78913e7e700ddd1a99301},
  isbn = {9780821849743 0821849743},
  keywords = {ai4science neuralpdes pdes theory},
  publisher = {American Mathematical Society},
  refid = {465190110},
  timestamp = {2022-10-12T17:19:51.000+0200},
  title = {Partial differential equations},
  year = 2010
} 
@misc{rakhlin2018consistency,
      title={Consistency of Interpolation with Laplace Kernels is a High-Dimensional Phenomenon}, 
      author={Alexander Rakhlin and Xiyu Zhai},
      year={2018},
      eprint={1812.11167},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@book{10.5555/1941882,
author = {Szeliski, Richard},
title = {Computer Vision: Algorithms and Applications},
year = {2010},
isbn = {1848829345},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
edition = {1st},
abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of recipes, this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features: structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses; presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects; provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory; suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book; supplies supplementary course material for students at the associated website, http://szeliski.org/Book/. Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.}
}
@ARTICLE{4309918,
  author={Rabiner, L. R. and Gold, B. and Yuen, C. K.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Theory and Application of Digital Signal Processing}, 
  year={1978},
  volume={8},
  number={2},
  pages={146-146},
  keywords={Digital signal processing;Books;Gold;Fast Fourier transforms;Discrete Fourier transforms;Signal processing algorithms;Hardware;Fourier series;Digital filters;Filtering},
  doi={10.1109/TSMC.1978.4309918}}
@misc{ wiki:taylor-series,
  author = {{Taylor series}},
  title = "Taylor series --- {W}ikipedia{,} The Free Encyclopedia",
  year = "2024",
  url = "https://en.wikipedia.org/wiki/Taylor_series",
  note = "[Online; accessed 18-May-2024]"
}
@misc{tai2016convolutional,
      title={Convolutional neural networks with low-rank regularization}, 
      author={Cheng Tai and Tong Xiao and Yi Zhang and Xiaogang Wang and Weinan E},
      year={2016},
      eprint={1511.06067},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{10.1007/s11554-023-01274-y,
author = {Zhang, Meng and Liu, Fei and Weng, Dongpeng},
title = {Speeding-up and compression convolutional neural networks by low-rank decomposition without fine-tuning},
year = {2023},
issue_date = {Aug 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-023-01274-y},
doi = {10.1007/s11554-023-01274-y},
abstract = {With the rapid development of convolutional neural network (CNN), the accuracy of CNN has been significantly improved, which also brings great challenges to the deployment of mobile terminals or embedded devices with limited resources. Recently, significant achievements have been made in compressing CNN through low-rank decomposition. Unlike existing methods that use the same decomposition form and decomposition strategy with fine-tuning based on singular value decomposition (SVD), our method uses different decomposition forms for different layers, and proposes decomposition strategies without fine-tuning. We present a simple and effective scheme to compress the entire CNN, which is called cosine similarity SVD without fine-tuning. For the AlexNet, our cosine similarity algorithm of rank selection takes 84\% of the time to find the rank compared with the bayesian optimization (BayesOpt) algorithm. After we tested various CNNs (AlexNet, VGG-16, VGG-19, and ResNet-50) on different data sets, experimental results show that the weight parameter drop can exceed 50\% when the accuracy loss is less than 1\% without fine-tuning. The floating point operations (FLOPs) drop is about 20\%, and the accuracy loss is less than 1\% without fine-tuning.},
journal = {J. Real-Time Image Process.},
month = {may},
numpages = {12},
keywords = {Rank selection, Low-rank decomposition, Neural network compression, Convolutional neural networks}
}
@misc{sui2024elrt,
      title={ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks}, 
      author={Yang Sui and Miao Yin and Yu Gong and Jinqi Xiao and Huy Phan and Bo Yuan},
      year={2024},
      eprint={2401.10341},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{ding2022scaling,
      title={Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs}, 
      author={Xiaohan Ding and Xiangyu Zhang and Yizhuang Zhou and Jungong Han and Guiguang Ding and Jian Sun},
      year={2022},
      eprint={2203.06717},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}
@misc{rozière2024code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}